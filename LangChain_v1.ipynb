{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alvaro-Cavadia/LangChain_Python/blob/main/LangChain_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![langchain sin color (1).png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "pK6NXfy8OKb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LangChain**"
      ],
      "metadata": {
        "id": "gOtBlP_uPGkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain es un framework diseñado para simplificar el desarrollo de aplicaciones impulsadas por Modelos de Lenguaje Grandes (LLMs), permitiendo a los desarrolladores encadenar componentes de forma modular, como el historial de conversaciones, el acceso a datos externos, y la ejecución de herramientas, facilitando la creación de asistentes sofisticados con memoria y capacidad de razonamiento complejo."
      ],
      "metadata": {
        "id": "b1FC7Ft5gEVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración de la API_KEY"
      ],
      "metadata": {
        "id": "tL8ANfemPQe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"xxxxxxxxx\"\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
      ],
      "metadata": {
        "id": "C1cKDRyR30Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalación de LangChain y OpenAI**"
      ],
      "metadata": {
        "id": "CEvr75k2Txml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain"
      ],
      "metadata": {
        "id": "whnZWOOwRF4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80298aae-3335-462c-f126-b3fb8728093a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.4 (from langchain)\n",
            "  Downloading langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (0.4.42)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.4->langchain) (4.15.0)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_prebuilt-1.0.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.4->langchain) (3.0.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
            "Downloading langchain-1.0.5-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.8/93.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.4-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.4-py3-none-any.whl (34 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "Successfully installed langchain-1.0.5 langchain-core-1.0.4 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.4 langgraph-sdk-0.2.9 ormsgpack-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-openai"
      ],
      "metadata": {
        "id": "6tGZLaqsRRq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48109cd-7c36-414c-da33-d4e0a7fa1fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.0.4)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.4.42)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.11.10)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-1.0.2-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Componentes**"
      ],
      "metadata": {
        "id": "xYp_eDWEUdIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelos"
      ],
      "metadata": {
        "id": "RRrlpy9EBDHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los LLM son potentes herramientas de IA que pueden interpretar y generar texto como los humanos. Son lo suficientemente versátiles como para redactar contenido, traducir idiomas, resumir y responder preguntas sin necesidad de formación especializada para cada tarea.\n",
        "\n",
        "Los modelos son el motor de razonamiento de los agentes . Impulsan su proceso de toma de decisiones, determinando qué herramientas utilizar, cómo interpretar los resultados y cuándo proporcionar una respuesta final.\n",
        "\n"
      ],
      "metadata": {
        "id": "0WJ2TIAjBJN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inicialización de un modelo"
      ],
      "metadata": {
        "id": "CTw0vQCVKFPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La forma más fácil de comenzar con un modelo independiente en LangChain es inicializar `init_chat_model` de un proveedor de su elección (ejemplos  continuación):"
      ],
      "metadata": {
        "id": "_dM2y1ndJp5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "model = init_chat_model(\"gpt-xxx\")"
      ],
      "metadata": {
        "id": "nw3q2BKvA9ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando el metodo `invoke` se llama al modelo para hacerle una consulta\n",
        "\n"
      ],
      "metadata": {
        "id": "yP_zPBvMKaTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(\"¿Qué es optimización?\")"
      ],
      "metadata": {
        "id": "MA5Uy0SVKUMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "para acceder solo al texto de la respuesta, se debe usar el atributo `.content`, de lo contrario, tambien se obtendrá información adicional (metadatos como el uso de tokens)"
      ],
      "metadata": {
        "id": "ExfBb35NXxT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "id": "YyqPl3NiSM_0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "f4354348-94f1-4833-92d5-a7b9ec2c7dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'La optimización es un proceso matemático y computacional que busca encontrar el mejor resultado o la mejor solución posible dentro de un conjunto de restricciones y condiciones. Se utiliza en diversas áreas, como la economía, la ingeniería, la logística y la inteligencia artificial, entre otras.\\n\\nExisten dos componentes clave en la optimización:\\n\\n1. **Función objetivo**: Es la función que se desea maximizar o minimizar. Por ejemplo, maximizar las ganancias, minimizar costos, o minimizar el tiempo de producción.\\n\\n2. **Restricciones**: Son las condiciones o limitaciones que deben cumplirse dentro del problema. Pueden ser de naturaleza física, económica, técnica, etc.\\n\\nLa optimización puede ser de muchos tipos y abarcar diversas técnicas, como la programación lineal, programación no lineal, algoritmos genéticos, entre otros.\\n\\nEn resumen, la optimización tiene como objetivo encontrar la mejor solución posible bajo un conjunto de restricciones, maximizar o minimizar una función objetivo en un contexto específico.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parámetros"
      ],
      "metadata": {
        "id": "t9Fjde2F5ag0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un modelo de chat acepta parámetros que permiten configurar su comportamiento. El conjunto completo de parámetros admitidos varía según el modelo y el proveedor, pero los estándar incluyen:\n",
        "\n",
        "\n",
        "*   `model` el nombre o identificador del modelo específico que desea utilizar con un proveedor.\n",
        "*   `api_key` clave necesaria para autenticarse con el proveedor del modelo. Generalmente se emite al registrarse para acceder al modelo. A menudo se accede configurando unavariable de entorno.\n",
        "*   `temperature` Controla la aleatoriedad de la salida del modelo. Un número alto hace que las respuestas sean más creativas; un número bajo las hace más deterministas.\n",
        "*   `timeout` El tiempo máximo (en segundos) para esperar una respuesta del modelo antes de cancelar la solicitud.\n",
        "*   `max_tokens` Limita el número total de tokens en la respuesta, controlando efectivamente cuánto tiempo puede durar la salida.\n",
        "*   `max_retries` La cantidad máxima de intentos que realizará el sistema para reenviar una solicitud si falla debido a problemas como tiempos de espera de la red o límites de velocidad.\n"
      ],
      "metadata": {
        "id": "akjMM4um5sSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = init_chat_model(\n",
        "    \"gpt-xxx\",\n",
        "    temperature=0.7,\n",
        "    timeout=30,\n",
        "    max_tokens=300,\n",
        ")"
      ],
      "metadata": {
        "id": "EhfsSdh97xVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mensajes"
      ],
      "metadata": {
        "id": "Efc8OkUtL9T8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los mensajes son la unidad fundamental de contexto para los modelos en LangChain. Representan la entrada y la salida de los modelos, y contienen tanto el contenido como los metadatos necesarios para representar el estado de una conversación al interactuar con un LLM.\n",
        "\n",
        "\n",
        "*   **Indicaciones de texto** son cadenas, ideales para tareas de generación sencillas en las que no es necesario conservar el historial de conversaciones.\n",
        "*   **Indicaciones de mensajes** Alternativamente, puede pasar una lista de mensajes al modelo proporcionando una lista de objetos de mensaje (forma nativa de LangChain).\n",
        "*   **Formato de diccionario** También puede especificar mensajes directamente en el formato de diccionario, el cual es el que comunmente se usa para las APIs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AqvsiR-f4Bgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Indicaciones de texto"
      ],
      "metadata": {
        "id": "zAJlTe5Y-qr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es basicamente lo que se hizo al inicio."
      ],
      "metadata": {
        "id": "YjXOAPXeW4-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#response = model.invoke(\"¿Qué es optimización?\")"
      ],
      "metadata": {
        "id": "dprhlOJ_-n4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Indicaciones de mensajes"
      ],
      "metadata": {
        "id": "v4vtkAgl-xdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"Eres un experto en optimización\"), # Esto le dice al LLM que debe asumir el rol de un experto en optimización\n",
        "    HumanMessage(\"¿Qué es una restricción?\"),         # Esta es la petición real del usuario.\n",
        "    AIMessage(\"Según Hillier y Lieberman ...\")        # Este es un fragmento de respuesta anterior (o una respuesta parcial) del modelo.\n",
        "]\n",
        "response = model.invoke(messages)"
      ],
      "metadata": {
        "id": "ZbLCzyXh_dtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "id": "_bzjkaeHS4zl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "084ae1ca-4128-4a26-efb0-d587e7d90e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Una restricción, en el contexto de la optimización y la programación matemática, se refiere a una limitación o condición que debe cumplirse en un problema específico. Estas restricciones pueden ser de diferentes tipos, como:\\n\\n1. **Restricciones de igualdad**: Imponen que dos expresiones sean iguales. Por ejemplo, \\\\(x + y = 10\\\\).\\n\\n2. **Restricciones de desigualdad**: Establecen que una expresión debe ser mayor o menor que otra. Por ejemplo, \\\\(x - y \\\\leq 5\\\\) o \\\\(x + y \\\\geq 3\\\\).\\n\\n3. **Restricciones de no negatividad**: Especifican que ciertas variables deben ser mayores o iguales a cero, como \\\\(x \\\\geq 0\\\\).\\n\\nLas restricciones son fundamentales en la formulación de problemas de optimización, ya que definen el espacio de soluciones posibles y ayudan a encontrar la mejor solución bajo las condiciones dadas. En un problema típico de programación lineal, por ejemplo, se busca maximizar o minimizar una función objetivo sujeta a un conjunto de restricciones.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Formato de diccionario"
      ],
      "metadata": {
        "id": "QA38UsDH-51K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Se obtendrá la misma respuesta que la anterior\n",
        "\n",
        "# messages = [\n",
        "#     {\"role\": \"system\", \"content\": \"Eres un experto en optimización\"},\n",
        "#     {\"role\": \"user\", \"content\": \"¿Qué es una restricción?\"},\n",
        "#     {\"role\": \"assistant\", \"content\": \"Según Hillier y Lieberman ...\"}\n",
        "# ]\n",
        "# response = model.invoke(messages)"
      ],
      "metadata": {
        "id": "HRdF1I1WC1wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Se obtendrá la misma respuesta que la anterior\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Eres un metematico que no quiere resolver sumas manualmente\"},\n",
        "    {\"role\": \"user\", \"content\": \"¿cuanto es 5 + 8 ?\"}\n",
        "]\n",
        "response = model.invoke(messages)"
      ],
      "metadata": {
        "id": "VE0tAoW2qCBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tipos de mensajes**"
      ],
      "metadata": {
        "id": "nDvfcLrMEqCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existen diferentes tipos de mensajes:\n",
        "\n",
        "*   `SystemMessage` : le indica al modelo cómo comportarse y proporciona contexto para las interacciones.\n",
        "\n",
        "*    `HumanMessage` : representa la entrada del usuario y las interacciones con el modelo.\n",
        "*   `AIMessage`  : respuestas generadas por el modelo, incluido contenido de texto, llamadas de herramientas y metadatos\n",
        "\n",
        "*   `ToolMessage` : representa las salidas de las llamadas de herramienta (se explicarà en la secciòn de herramientas)\n",
        "\n"
      ],
      "metadata": {
        "id": "BGSKu1vyYVxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El uso de los mensajes puede llegar a ser complejo, pero inicialmente es suficiente con conocer lo mencionado en indicaciones de mensaje y formato de diccionario."
      ],
      "metadata": {
        "id": "UWTGOAxxbPOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Herramientas"
      ],
      "metadata": {
        "id": "_dGicNhLcEv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las herramientas son componentes que los agentes invocan para realizar acciones. Amplían las capacidades del modelo al permitirles interactuar con el mundo mediante entradas y salidas bien definidas. Las herramientas encapsulan una función invocable y su esquema de entrada. Estos se pueden pasar a modelos de chat compatibles , lo que permite al modelo decidir si invocar una herramienta y con qué argumentos."
      ],
      "metadata": {
        "id": "r5f_QSSBclJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Crear Herramientas**"
      ],
      "metadata": {
        "id": "uC5OAj4efR5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La forma más sencilla de crear una herramienta es con `@tool`. De forma predeterminada, la cadena de documentación de la función se convierte en la descripción de la herramienta, lo que ayuda al modelo a comprender cuándo usarla."
      ],
      "metadata": {
        "id": "TM58X6IZg6qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "# Herramienta\n",
        "\n",
        "@tool\n",
        "def sumar_numeros(a: int, b: int) -> str:\n",
        "    \"\"\"Suma dos números enteros (A y B). Útil cuando se necesita realizar una operación aritmética de suma.\"\"\"\n",
        "    resultado = a + b\n",
        "    return f\"El resultado de sumar {a} y {b} es: {resultado}\"\n",
        "\n",
        "print(sumar_numeros.invoke({\"a\": 15, \"b\": 27}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udpyta4y50YR",
        "outputId": "55ac5ca0-a725-45b6-b660-b68dfb406a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El resultado de sumar 15 y 27 es: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   La herramienta `sumar_numeros` recibe como argumentos a y b\n",
        "*   Tiene una descripción, la cual es util para el razonamiento del modelo, el cual decidirá si usarla.\n",
        "*   `sumar_numeros.invoke` llama a la función y debe recibir un diccionario con los parametos de la función.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TwO9DHP_7eI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Llamar herramientas usando un modelo**"
      ],
      "metadata": {
        "id": "F26r-A5R2y2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se pueden usar las herramientas con un modelo sin implementar un agente.\n",
        "\n",
        "Para que las herramientas definidas estén disponibles para un modelo, debe vincularlas mediante bind_tools(). En invocaciones posteriores, el modelo puede optar por llamar a cualquiera de las herramientas vinculadas según sea necesario."
      ],
      "metadata": {
        "id": "cerZhfmW3CqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_tools = model.bind_tools([sumar_numeros])"
      ],
      "metadata": {
        "id": "VRlEnsO69W9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso `response` no generará directamente la respuesta, pero se encargará de dos cosas importantes:\n",
        "\n",
        "\n",
        "*   Analizar el contenido del texto proporcionado.\n",
        "*   Decidir si utilizar la herramienta disponible, y en caso de aplicar la herramienta, debe proporcionar los parametros para `sumar_numeros`.\n",
        "\n"
      ],
      "metadata": {
        "id": "USpsOZcMES6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model_with_tools.invoke(\"Devuelve el resultado de la suma de 5 y 8\")\n"
      ],
      "metadata": {
        "id": "px9UW8Cw-PdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVvdA8sJMn6o",
        "outputId": "57c468cd-819f-4395-ea89-145d656c59fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 81, 'total_tokens': 102, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CV55146vHSMUKc7WakmWGdofXCoqn', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--0b20da74-7dc1-40df-874e-c7bd5cc038d5-0', tool_calls=[{'name': 'sumar_numeros', 'args': {'a': 5, 'b': 8}, 'id': 'call_c70djQcbsC31jjVEwGOHpl31', 'type': 'tool_call'}], usage_metadata={'input_tokens': 81, 'output_tokens': 21, 'total_tokens': 102, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`response.tool_calls` es una lista que contiene un diccionario con la información relevante de la función `Sumar_numeros`. Se accede aellos mediante un ciclo para verificar si realmente se está llamando la función y que le está pasando el modelo."
      ],
      "metadata": {
        "id": "PhLiOajbGf41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tool_call in response.tool_calls:\n",
        "    print(f\"Tool: {tool_call['name']}\")\n",
        "    print(f\"Args: {tool_call['args']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADioezdYADKe",
        "outputId": "86a4dd57-a355-4487-a340-a53927de964c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool: sumar_numeros\n",
            "Args: {'a': 5, 'b': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutamos la función Python usando los argumentos extraídos\n",
        "tool_result = sumar_numeros.invoke(tool_call['args'])\n",
        "\n",
        "print(\"\\n--- Ejecución de la Tool ---\")\n",
        "print(f\"Resultado de la Tool (Observation): {tool_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUB-9FD0Cqll",
        "outputId": "b420f4a5-50d6-405a-ecca-85997ca0fcb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Ejecución de la Tool ---\n",
            "Resultado de la Tool (Observation): El resultado de sumar 5 y 8 es: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Bucle de ejecución de herramientas**"
      ],
      "metadata": {
        "id": "Ak4fq4QLJ_Np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuando un modelo devuelve llamadas a herramientas, es necesario ejecutarlas y devolverle los resultados. Esto crea un bucle de conversación donde el modelo puede usar los resultados de las herramientas para generar su respuesta final. LangChain incluye abstracciones de agente que gestionan esta orquestación."
      ],
      "metadata": {
        "id": "1dT2ihGmKVZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo parecido al anterior\n",
        "model_with_tools = model.bind_tools([sumar_numeros])\n",
        "\n",
        "# El modelo genera la llamada a la herramienta\n",
        "messages = [{\"role\": \"user\", \"content\": \"Devuelve el resultado de la suma de 10 y 12\"}]\n",
        "ai_msg = model_with_tools.invoke(messages)\n",
        "# Guarda la llamada de la herramienta en messages\n",
        "messages.append(ai_msg)\n",
        "\n",
        "# Ciclo de ejecución\n",
        "for tool_call in ai_msg.tool_calls:\n",
        "    # Ejecución de la herraienta con los argumentos generados\n",
        "    tool_result = sumar_numeros.invoke(tool_call)\n",
        "    # Guarda el resultado de la herramenta en messages\n",
        "    messages.append(tool_result)\n",
        "\n",
        "# Los resultados regresan al modelo para que este genere una respuesta basandose en todo el contenido de messages\n",
        "final_response = model_with_tools.invoke(messages)\n",
        "# .test asegura que se imprima solo el texto de la respuesta\n",
        "print(final_response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QA-MEXJJ9Yu",
        "outputId": "928caa70-49e6-4537-fd56-6041f64d6bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El resultado de sumar 10 y 12 es: 22.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parece que funciona igual que cuando se genera la respuesta desde la herramienta, pero en este caso la respuesta final viene del modelo, y en caso de haber varias herramientas o ser cosas mas complejas, el modelo puede intervenir y hacer ajustes"
      ],
      "metadata": {
        "id": "EWXGHkDDOGtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Definición de esquema avanzado**\n"
      ],
      "metadata": {
        "id": "Qc9fX6QO-O4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuando las entradas son complejas se requieren formatos especificos. En este caso se utiliza un JSON para definir los argumentos de la herramienta. Para esto se debe crear una clase de `pydantic`."
      ],
      "metadata": {
        "id": "3butAH_aP7ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain.tools import tool\n",
        "\n",
        "# Definición del esquema que sigue la herramienta\n",
        "class SumaInput(BaseModel):\n",
        "    \"\"\"Esquema de entrada para la operación de suma.\"\"\"\n",
        "    a: int = Field(description=\"El primer número entero (primer sumando).\")\n",
        "    b: int = Field(description=\"El segundo número entero (segundo sumando).\")\n",
        "\n",
        "# Herramienta\n",
        "@tool (args_schema=SumaInput) # args_schema=SumaInput Hace que la herramienta cumpla con el esquema de SumaInput\n",
        "def sumar_numeros(a: int, b: int) -> str:\n",
        "    \"\"\"Suma dos números enteros (A y B). Útil cuando se necesita realizar una operación aritmética de suma.\"\"\"\n",
        "    resultado = a + b\n",
        "    return f\"El resultado de sumar {a} y {b} es: {resultado}\"\n"
      ],
      "metadata": {
        "id": "48-q9DSCRhut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ynOKEr0kcJN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo parecido al anterior\n",
        "model_with_tools = model.bind_tools([sumar_numeros])\n",
        "\n",
        "# El modelo genera la llamada a la herramienta\n",
        "messages = [{\"role\": \"user\", \"content\": \"Devuelve el resultado de la suma de 45 y 12\"}]\n",
        "ai_msg = model_with_tools.invoke(messages)\n",
        "# Guarda la llamada de la herramienta en messages\n",
        "messages.append(ai_msg)\n",
        "\n",
        "# Ciclo de ejecución\n",
        "for tool_call in ai_msg.tool_calls:\n",
        "    # Ejecución de la herraienta con los argumentos generados\n",
        "    tool_result = sumar_numeros.invoke(tool_call)\n",
        "    # Guarda el resultado de la herramenta en messages\n",
        "    messages.append(tool_result)\n",
        "\n",
        "# Los resultados regresan al modelo para que este genere una respuesta basandose en todo el contenido de messages\n",
        "final_response = model_with_tools.invoke(messages)\n",
        "# .test asegura que se imprima solo el texto de la respuesta\n",
        "print(final_response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT9nPi2JRTc_",
        "outputId": "8c79cb4f-935f-416f-e1b0-60dfecdafc83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El resultado de sumar 45 y 12 es 57.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Memoria**"
      ],
      "metadata": {
        "id": "1DI3bGApQR2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La memoria es un sistema que recuerda información sobre interacciones previas. Para los agentes de IA, la memoria es crucial porque les permite recordar interacciones anteriores, aprender de la retroalimentación y adaptarse a las preferencias del usuario. A medida que los agentes abordan tareas más complejas con numerosas interacciones con el usuario, esta capacidad se vuelve esencial tanto para la eficiencia como para la satisfacción del usuario.\n",
        "La memoria a corto plazo permite que su aplicación recuerde interacciones previas dentro de un mismo hilo o conversación."
      ],
      "metadata": {
        "id": "jkUbez60AnuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Uso de la memoria a corto plazo**"
      ],
      "metadata": {
        "id": "pI71vlc8Avk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente ejemplo contiene un pequeño agente para ilustrar el uso de la memoria a corto plazo.\n",
        "\n",
        "la funciòn principal de `checkpointer` es guardar los estados de la conversaciòn, `InMemorySaver()` indica que se guardarà en la ram.\n",
        "\n",
        "`{\"configurable\": {\"thread_id\": \"1\"}}` es la forma en la que se puede especificar un id de usuario, usando `\"thread_id\"`. Esto permite multiples interacciones con varios usuarios y no mezclar las conversaciones.\n",
        "\n",
        "La memoria a corto plazo sirve para que el agente recurde las interacciones cuando se esta ejecutando, pero una vez cerrado el programa, se elimina el historial."
      ],
      "metadata": {
        "id": "JJ1EhWRVBGcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "\n",
        "agent = create_agent(\n",
        "    \"gpt-xxx\",\n",
        "    checkpointer=InMemorySaver(),\n",
        ")\n",
        "\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n",
        "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
        ")\n",
        "\n",
        "for message in response['messages']:\n",
        "    print(message.content)\n"
      ],
      "metadata": {
        "id": "JwwQaSv_cAU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21c255a-10f8-4efe-c714-8412c06a1b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! My name is Bob.\n",
            "Hi Bob! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente ejemplo muestra como el agente recurda el nombre de un usuario.\n",
        "\n",
        "Para tener la respuesta de la ultima pregunta usamos `[\"messages\"][-1]` y `pretty_print()` para que el mensaje de ia sea en un formato legible para humanos."
      ],
      "metadata": {
        "id": "j2Sa6QupKdJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_agent(\n",
        "    \"gpt-xxx\",\n",
        "    checkpointer=InMemorySaver()\n",
        ")\n",
        "\n",
        "config ={\"configurable\": {\"thread_id\": \"1\"}}\n",
        "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
        "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
        "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
        "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
        "\n",
        "final_response[\"messages\"][-1].pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWK8UgeHIfvp",
        "outputId": "27520dd4-a582-4874-ddb8-d50c5b0b45cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob! How can I help you today, Bob?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Agente**"
      ],
      "metadata": {
        "id": "32MH0pccLclR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los agentes combinan modelos de lenguaje con herramientas para crear sistemas que pueden razonar sobre las tareas, decidir qué herramientas usar y trabajar iterativamente para encontrar soluciones.\n",
        "`create_agent` Proporciona una implementación de agente lista para producción."
      ],
      "metadata": {
        "id": "BjihjsaELged"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo bàsico de agente\n",
        "agent = create_agent(\n",
        "    \"gpt-xxx\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "1dVDd0X5LzYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplo de agente**"
      ],
      "metadata": {
        "id": "9oIgU9atb2gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos visto lo necesario para la creaciòn de agentes funcionales, por tal razòn no es necesario explicar su creaciòn paso a paso."
      ],
      "metadata": {
        "id": "wd5joBGVMR3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "from langchain.agents import create_agent\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.tools import tool, ToolRuntime\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "\n",
        "# Define system prompt\n",
        "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
        "\n",
        "You have access to two tools:\n",
        "\n",
        "- get_weather_for_location: use this to get the weather for a specific location\n",
        "- get_user_location: use this to get the user's location\n",
        "\n",
        "If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n",
        "\n",
        "# Define context schema\n",
        "@dataclass\n",
        "class Context:\n",
        "    \"\"\"Custom runtime context schema.\"\"\"\n",
        "    user_id: str\n",
        "\n",
        "# Define tools\n",
        "@tool\n",
        "def get_weather_for_location(city: str) -> str:\n",
        "    \"\"\"Get weather for a given city.\"\"\"\n",
        "    return f\"It's always sunny in {city}!\"\n",
        "\n",
        "@tool\n",
        "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
        "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
        "    user_id = runtime.context.user_id\n",
        "    return \"Florida\" if user_id == \"1\" else \"SF\"\n",
        "\n",
        "# Configure model\n",
        "model = init_chat_model(\n",
        "    \"gpt-xxx\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Define response format\n",
        "@dataclass\n",
        "class ResponseFormat:\n",
        "    \"\"\"Response schema for the agent.\"\"\"\n",
        "    # A punny response (always required)\n",
        "    punny_response: str\n",
        "    # Any interesting information about the weather if available\n",
        "    weather_conditions: str | None = None\n",
        "\n",
        "# Set up memory\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "# Create agent\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    system_prompt=SYSTEM_PROMPT,\n",
        "    tools=[get_user_location, get_weather_for_location],\n",
        "    context_schema=Context,\n",
        "    response_format=ResponseFormat,\n",
        "    checkpointer=checkpointer\n",
        ")\n",
        "\n",
        "# Run agent\n",
        "# `thread_id` is a unique identifier for a given conversation.\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n",
        "    config=config,\n",
        "    context=Context(user_id=\"1\")\n",
        ")\n",
        "\n",
        "print(response['structured_response'])\n",
        "# ResponseFormat(\n",
        "#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n",
        "#     weather_conditions=\"It's always sunny in Florida!\"\n",
        "# )\n",
        "\n",
        "\n",
        "# Note that we can continue the conversation using the same `thread_id`.\n",
        "response = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
        "    config=config,\n",
        "    context=Context(user_id=\"1\")\n",
        ")\n",
        "\n",
        "print(response['structured_response'])\n",
        "# ResponseFormat(\n",
        "#     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n",
        "#     weather_conditions=None\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbryMsN2MRO4",
        "outputId": "d93d6966-ae8f-4fa8-dfe8-f10b80df7601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResponseFormat(punny_response=\"Looks like Florida is putting on its best show with sunny skies! It's a bright day to soak up some rays!\", weather_conditions='sunny')\n",
            "ResponseFormat(punny_response=\"You're welcome! I'm always here to brighten your day with a little weather pun!\", weather_conditions=None)\n"
          ]
        }
      ]
    }
  ]
}